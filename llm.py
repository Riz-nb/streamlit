import os
import requests
from dotenv import load_dotenv
import pandas as pd
import streamlit as st
import evaluate
load_dotenv()
API_KEY = os.getenv("GROQ_API_KEY")

def summarize_text(prompt, model="llama-3.1-8b-instant"):
    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {os.getenv('GROQ_API_KEY')}",
        "Content-Type": "application/json",
    }
    data = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.1
    }

    response = requests.post(url, headers=headers, json=data)

    try:
        response_json = response.json()
    except Exception:
        return "Invalid JSON response."

    if response.status_code != 200:
        return f"API Error: {response.status_code} - {response_json.get('error', {}).get('message', 'Unknown error')}"

    if "choices" not in response_json:
        return f"Invalid response format: {response_json}"

    return response_json["choices"][0]["message"]["content"]


def ask_question(context, question, model):
    import os
    import requests

    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {os.getenv('GROQ_API_KEY')}",
        "Content-Type": "application/json",
    }

    prompt = f"""You are a helpful assistant. Use the context below to answer the user's question.can?

Context:
\"\"\"{context}\"\"\"

Question: {question}
If the answer is not found in the context, say sorry its not in the context of bishops calender."""

    data = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.2
    }

    try:
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()
        response_json = response.json()

        return response_json.get("choices", [{}])[0].get("message", {}).get("content", "❌ No content returned").strip()

    except Exception as e:
        return f"❌ {model} failed: Can you clarify your question"

def fuse_summaries(summaries, model="llama-3.1-8b-instant"):
    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }

    content = "Here are the summaries:\n\n"
    for i, summary in enumerate(summaries, 1):
        content += f"{i}. {summary}\n\n"
    content += "Please fuse these into one accurate, non-repetitive summary:"

    messages = [
        {
            "role": "system",
            "content": "You are a helpful assistant. You are given several summaries generated by different models for the same document. Your task is to combine them into a single, high-quality summary that captures the most important points and avoids repetition."
        },
        {
            "role": "user",
            "content": content
        }
    ]

    data = {
        "model": model,
        "messages": messages
    }

    response = requests.post(url, headers=headers, json=data)
    try:
        return response.json()["choices"][0]["message"]["content"]
    except KeyError:
        print(" done:", response.text)
        return "done."
    
def evaluate_summaries_with_bertscore(predictions, reference, lang="en"):
    bertscore = evaluate.load("bertscore")
    if isinstance(reference, str):
        references = [reference] * len(predictions)
    else:
        references = reference
    results = bertscore.compute(predictions=predictions, references=references, lang=lang)
    return results["f1"]